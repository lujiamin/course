## 大数据时代的算法
### 刘凡平  
#### chapter 1 算法基础
- 基础算法分析类型
  - 分治法——分而治之，将一个复杂问题拆解为两个或者多个相同的子问题
    - 需满足：1)问题可以分解;2)问题在小规模情况下成立;3)问题拆分后的子问题可以合并
    - 常见例子：二分搜索，归并排序，快速排序
  - 动态规划——划分问题后，存在重复子问题，将其放在结果表中，下一次计算从中提取
    - 条件：1)最有子结构性质——一个问题的最优解包含的所有问题的解也是最优的;2)子问题重叠性质
    - 常见例子：背包问题，最长公共子序列
  - 回溯法——在所有的解空间内，按照深度优先尝试的原则。在选择求解目标过程中会利用当前条件与目标的最优匹配选择进行尝试，尝试失败，回到上一步继续选择最后匹配选择进行
  - 分支界限法——以广度优先或者以最小代价优先。与回溯法的区别就在此
  - 贪心法——当前求解的过程在当前的条件下是最好的    
- 算法性能分析
  - 时间复杂度
  - 空间复杂度
- 概率论与数理统计基础
  - 二项分布、超几何分布、泊松分布、指数分布、正态分布
  - 先验概率、条件概率、联合概率
- 距离计算
  - 欧氏距离——n维空间中两个点之间的真实距离 d = [(x1-x2)^2+(y1-y2)^2]^(1/2)
  - 马氏距离——表示数值协方差距离，体现的是数据样本分布的距离。用于计算两个未知样本信息集合的相似度分析
  - 曼哈顿距离——两个点在坐标系上的绝对轴距总和 d = |x1-x2|+|y1-y2|
  - 切比雪夫距离—— d = lim{[(x1-y1)^p+(x2-y2)^p+...]^(1/p)}，p->+∞
  - 闵式距离——闵可夫斯基距离，对于两个n维向量A(x1,x2,x3...)和B(y1,y2,y3...),d = [(x1-y1)^p+(x2-y2)^p+...]^(1/p)，p=1，即是曼哈顿距离;p=2，即是欧式距离，p趋于无穷大，即是切比雪夫距离。
  - 海明距离——两个等长字符串对应位置不同字符的个数
- 排序算法
  - 快速排序
  - 归并排序
  - 堆排序
  - 基数排序
  - 外排序——数据无法一次性加载在内存中，主要使用归并排序，且目前的数据量都大于内存容量，可以进行并行化处理和分布式化处理来改进
- 字符压缩编码
  - 哈夫曼编码——通过变长编码的方式对数据进行编码，通过权值的评估。
  - 香浓-范诺编码——按符号出现的概率由大到小进行排列，将其分为两组，使其概率之和尽可能相等，约定左边为0，右边为1，再对每组进行上述迭代，直至每组只剩一个数据
  - 哈夫曼编码的优势较为明显，香浓-范诺编码产生的并不是最优前缀码
#### chapter 2 数据查找与资源分配算法
- 数值查找算法
  - 二分搜索算法
  - 分块搜索算法
  - 哈希查找算法——哈希函数和哈希表的结构
- 字符串查找算法
  - KMP算法
  - Boyer-Moore算法
  - Sunday算法
- 海量数据的查找
  - 基于布隆过滤器查找
  - 倒排索引查找
- 银行家算法
- 背包问题
  - 0-1背包问题——动态规划问题
  ```java
  // 0-1背包问题。weight[]表示物品的重量，value[]表示物品的价值，totalWeight表示背包的总容量。数组下标从1开始
    public static int getMostValue(int weight[], int[] value, int totalWeight) {
        int num = weight.length - 1;// num个物品
        int dp[] = new int[totalWeight + 1];
        for (int i = 1; i <= num; i++) {
            for (int j = totalWeight; j >= 1; j--) {
                if (weight[i] <= j) {
                    dp[j] = (dp[j] < dp[j - weight[i]] + value[i]) ? dp[j - weight[i]] + value[i] : dp[j];
                }
            }
        }
        return dp[totalWeight];
    }
  ```
  - 部分背包问题——贪婪算法 
#### chapter 3 路径分析算法
- 基于Dijkstra算法的路径分析——用于计算一个点到其他所有节点的最短路径
- 基于Floyd算法的路径分析——多源点之间的最短路径
- 基于维比特算法的概率路径——应用于中文分词，天气预测等
- 最长公共子串(需连续)
```java
// 返回最长公共子串的长度
    public static int maxLength(String a, String b) {
        int m = a.length();
        int n = b.length();
        int dp[][] = new int[m + 1][n + 1];
        int ans = 0;

        for (int i = 1; i <= m; i++) {
            for (int j = 1; j <= n; j++) {
                // 等于左上角的值加1
                if (a.charAt(i - 1) == b.charAt(j - 1))
                    dp[i][j] = dp[i - 1][j - 1] + 1;
                if (dp[i][j] > ans) {
                    ans = dp[i][j];
                }
            }
        }
        return ans;
    }
```
- 最长公共子序列问题(通过回溯法将序列输出)
```java
// 求最长公共子序列，mark[][]为标记数组
    public static int maxLength(String a, String b, int mark[][]) {
        int m = a.length();
        int n = b.length();
        int dp[][] = new int[m + 1][n + 1];

        for (int i = 1; i <= m; i++) {
            for (int j = 1; j <= n; j++) {
                if (a.charAt(i - 1) == b.charAt(j - 1)) {
                    dp[i][j] = dp[i - 1][j - 1] + 1;
                    mark[i][j] = 0; // 左上角

                } else if (dp[i - 1][j] > dp[i][j - 1]) {
                    dp[i][j] = dp[i - 1][j];
                    mark[i][j] = 1; // 上面
                } else {
                    dp[i][j] = dp[i][j - 1];
                    mark[i][j] = -1; // 左面
                }
            }
        }
        return dp[m][n];
    }
// 回溯法输出
    public static void output(String a, int mark[][], int m, int n) {
        if (m == 0 && n == 0) {
            return; // 终止条件
        }
        if (mark[m][n] == 0) {
            output(a, mark, m - 1, n - 1);
            System.out.print(a.charAt(m - 1));
        } else if (mark[m][n] == 1) {
            output(a, mark, m - 1, n);
        } else {
            output(a, mark, m, n - 1);
        }
    }
```
#### chapter 4 相似度分析算法
- 海量网页相似度分析
- 基于jaccard相似系数的相似度计算——计算个体间的相似度
  - 文章相似度计算流程
    - 1) 分词处理，网页A和网页B进行分词
    - 2) 将网页A内容对应的词语当做一个集合A
    - 3) 计算集合A和集合B的交集，并集
    - 4) 计算jaccard系数 = 交集数量 / 并集数量
  - 狭义的jaccard相似系数：交集和并集的比值
  - 广义的jaccard相似系数：EJ=(A×B)/(|A|^2+|B|^2-A×B)，其中A和B表示向量
- 基于MinHash的相似度算法
  - 依然基于jaccard相似系数，不同的是引入了哈希函数机制，用于解决数据量大的情况，精度有所下降
- 向量空间模型
  - 词袋模型——将词语混合装入集合，利用向量的大小表示词语在文中的词频。忽略词序和语法的文本信息处理
  ```
  A:小张 喜欢 打 篮球 和 打 羽毛球
  B:小李 喜欢 打 羽毛球
  对应的词袋：小张 喜欢 打 篮球 和 羽毛球 小李
  A:(1,1,2,1,1,1,0)
  B:(0,1,1,0,0,1,1)
  ```
  - TF-IDF算法(Term Frequency-Inverse Document Frequecy)
    - 核心思想：文件中每个词的重要性与它当前文件中出现的次数成正比，但与它在其他文件中出现的次数成反比
    - 词频：TF(i,j) = n(i,j)/∑n(k,j)，n(i,j)为该词在文件dj中出现的次数，分母为所有词在该文件中出现的次数
    - 逆向文件频率：IDF(i) = lg(D/ti∈dj)，D表示所有文件的总数，分母表示包含该词ti的文件数目
    - TF-IDF：TF(i,j)×IDF(i)，表示该词在该文件中的TF-IDF权值
    - 在长文本中效果会比较好。短文本用TextRank算法
- 基于余弦相似度算法的相似度分析
  - 公式解析：向量a(x1,y1)，向量b(x2,y2)，则cosθ = a·b/|a||b| 
  ```
  A:数据 价值 是 一种 数据 艺术
  B:算法 价值 是 一种 算法 艺术
  向量集：数据 算法 价值 是 一种 艺术 
  A:(2,0,1,1,1,1)
  B:(0,2,1,1,1,1)
  cosθ = 4/8 = 0.5，两者余弦相似度为0.5
  ```
  - 传统情况下是一个可取的公式，但是存在问题：余弦相似性两两计算，计算复杂度高；文本少的时候，因为一两个关键词波动较大
- 基于语义主题模型的相似度算法
  - 两个文本很少有重复或者相似的词语，则需从语义关联性进行分析。语义都有对应的主题，主题是对事物的实体或某方面的概念性总结
  - 例如：A：北京天气，B：首都天气，利用余弦相似性计算的相似度为0.5，利用主题模型计算应大于0.5，如果将北京和首都视为相同主题，则相似度为1
  - 主题模型建模过程
    - LDA(latent Dirichlet Allocation):对一堆文章进行自动化聚类，一个主题可以理解为一个类别
    - pLSA(Probabilistic Latent Semantic Analysis):采用最大期望算法
  - 任意两个词语的相似度取决于词语之间的共性和个性组合
- 基于SimHash算法的指纹码
  - 计算流程(以“鼓励青年创新创业”)
    - 分词与权重计算——“鼓励 青年 创新 创业” 对应权重1，3,2,2
    - 哈希二进制计算——转换的结果是一个32位(或其他)的二进制值。例如将“鼓励”对应的哈希整型值“1283582”转换成二进制
    - 词语加权——将二进制值进行按位乘法，例如"青年"对应"0010",权重为3，加权后为"-3 -3 3 -3"
    - 合并累计——将文本词语的加权值进行合并，例如得到结果"-2 2 -2 2"
    - 降维输出——得到"0101"，小于0为0，大于0为1
  - 两个simHash值计算海明距离(二进制对应位不同的个数)，差值越小，相似度越高
#### chapter 5 数据分类算法
- 主要流程
  - 训练过程：从训练集中进行特征选取，对分类模型进行训练，从而形成分类器
  - 识别过程：对被识别的样本进行特征提取，然后利用分类器进行识别
- 基于朴素贝叶斯分类器
  - 有监督分类——首先需要一定量级的训练数据，通过对训练数据的特征提取，形成符合特征的分类模型，最终形成分类器，实现对数据的分类
  - 无监督分类——对分类器不事先进行先验知识学习，不进行参数训练，样本数据直接进行分类的过程
  - 贝叶斯定理
    - 公式：P(A|B)=P(B|A)*P(A)/P(B)
    - 信息由若干特征组成，将特征视为一个向量集W={w1,w2,w3...}，信息的分类也视为一个分类标记的集合C={c1,c2,,c3...}，则信息属于分类C的概率，P(C|W) = P(W|C)*P(C)/P(W)
  - 分类流程
    - 数据准备，语料库，确定特征属性
    - 分类器训练
    - 分类识别
  - 应用拓展：垃圾邮件识别
    - 使用标记(字词)与垃圾邮件、非垃圾邮件的关联，计算邮件是否是垃圾邮件的可能性  
    
    | 编号 | 特征内容(关键字) | 识别类型 |
    | - | - | - |
    | 1 | 创新 厚积薄发 | 非垃圾邮件 |
    | 2 | 创新 广告 发票 | 垃圾邮件 |
    | 3 | 财务 利润 | 非垃圾邮件 |
    | 4 | 创新 预算 减少 | 非垃圾邮件 |
    ```
    P(垃圾邮件) = 3/4 = 0.75
    P(非垃圾邮件) = 1/4 = 0.25
    针对非垃圾邮件：
    P(创新|非垃圾) = 2/3 = 0.67
    ...
    针对垃圾邮件：
    P(创新|垃圾邮件) = 1
    ...
    没个词出现的概率：
    P(创新) = 3/4 = 0.75
    P(预算) = 1/4 = 0.25
    ...
    推得每个词属于垃圾邮件和非垃圾邮件的概率：
    P(非垃圾|创新) = 0.67
    P(垃圾邮件|创新) = 0.33
    ...
    ```
  - 常用评价指标
    - 正确率、错误率
    - 精准度
    - 召回率
    - 鲁棒性——缺失数据、异常数据的情况下的处理能力
    - 计算复杂度
    - 模型描述的简洁度
- 基于AdaBoost分类器
  - 是一种有监督的学习方法，把若干个弱分类器组合成一个强分类器
  - 具体流程
    - 初始化训练数据的权值分布，n个样本权值都取为1/n
    - 进行多轮迭代，误差率ε为被分类错误的样本的权值之和。1) 用不同的阈值得到不同的误差率，选取误差率最小的阈值来设计弱分类器：Gm(x)
    - 计算Gm(x)的系数α，表示其在最终的强分类器中的重要程度。`α=(ln[(1-ε)/ε])/2`
    - 更新训练数据集的权值分布进行下一轮迭代，需要通过规范因子公式进行计算
    - 最后得到强分类器:`G(x) = sign(∑α·Gm(x))` 

    | 序号 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 |
    | - | - | - | - | - | - | - | - | - | - | - |
    | x | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |
    | y | 1 | 1 | 1 | -1 | -1 | -1 | 1 | 1 | 1 | -1 |
    ```
    1)第一轮迭代。取权值都为0.1，选2.5,5.5,8.5为阈值进行分类计算误差率，通过计算ε最小为0.3
    2)得到一个弱分类器，G1(x) = 1,x < 2.5;G1(x) = -1 ,x > 2.5，且α = 0.42
    3) f(x) = 0.42G1(x)
    4)进行第二轮迭代，最终组合成强分类器
    ```
- 基于支持向量机的分类器
  - 支持向量机(Support Vector Machine,SVM)是一种有监督的学习模型，属于一般化的线性分类器
  - 感知器是一种二类分类模型，主要针对线性可分的样本。是一种有监督的学习方式
- 基于K邻近算法的分类器
  - KNN(K-Nearest Neighbor)——通过以某个数据位中心，分析离其最近的K个邻居特征
  - 实例：电影观众兴趣发现
    - 1) 根据观众看过的电影，得到每个观众看过电影的向量(w1,w2,w3...)
    - 2) 计算观众之间的相似度，可以使用欧式距离或者余弦相似度
    - 3) 根据已知某X观众对某A电影的喜好，根据相似度推出其他观众对A电影的喜好程度
#### chapter 6 数据聚类算法
- 分类
  - 系统聚类法——又称层析聚类法
  - 逐步聚类法——也称快速聚类法，用于大样本之间，定义样本的中心点，将其他样本与初始的中心点计算距离进行聚簇，不断迭代直到聚簇中心不再改变。
  - 有序样品聚类法——将次序相邻的样本聚为一类
  - 模糊聚类法——基于模糊数学
- 距离计算方法
  - 最短距离法——两两之间计算距离，将距离最近的分为一类
  - 重心法——重心为属于该类的样本的平均值，例如将{2,3}的重心记为2.5
  - 动态聚类法

  | 用户 | 对A商品评分 | 对B商品 |
  | - | - | - |
  | 甲 | 5 | 3 |
  | 乙 | -1 | 1 |
  | 丙 | 1 | -2 |
  | 丁 | -3 | -2 |

  随机将甲、乙划为一类，丙丁划为一类，计算平均值
  | 聚类 | 对A平均分 | 对B平均分 |
  | - | - | - |
  | 甲、乙 | 2 | 2 |
  | 丙、丁 | -1 | -2 |
  
  然后计算甲距离第一类和第二类的距离，再次将甲划为第一类，依次进行
- 基于K-Means聚类方法
  - 是一种无监督机器学习算法
  - 流程
    - 初始选择k个点作为中心点
    - 计算各个点到k个中心点的距离，划分到距离最近的那个中心点，并且计算聚簇集合的中心点
    - 重复进行上述操作，直至中心点不在频繁波动
  - 缺陷
    - 对异常值、摇摆值比较敏感
    - 非常不适合均匀分布、数据界限不明晰的聚类
    - 初始中心点的选择对迭代的次数影响极大
    - 需要提前知道k的值
  - K-Means++
    - 在K-Means的基础上对初始点的选择前进行计算，使得初始点之间的距离尽可能大
  - K-中心点聚类算法
    - K-Medoids是对异常值造成的影响进行改进的算法
    - 在聚簇中心点的选取上，是计算当前集合中到集合内其他点距离之和最小的点作为中心点
    ```
    例：{1,3,7}
    1: d=2+6=8
    3: d=2+4=6
    7: d=6+4=10
    选择3作为中心点，再次进行聚类 
    ```
- 基于密度的DBSCAN算法
  - Density-Based Spatial Clustering of Applications with Noise
  - 概念
    - 直接密度可达：样本点q在核心点p的半径范围内
    - 密度可达：样本集合N中对象链P1,P2...Pn，如果P1=q，Pn=p，Pi+1在在核心点Pi的半径范围内，则p个q密度可达
    - 密度相连：A与P和Q都是密度可达，则P和Q密度相连
    - 核心点：在该点的指定半径范围内有超过指定数量的点，属于密度稠密区内部的点
    - 边界点：在该点的指定半径范围内有小于指定数量的点，属于密度稠密区边缘的点
    - 噪声点：不在核心点的领域内，且在指定半径范围内的点数也小于指定数量
- 基于BIRCH算法的聚类算法
  - 核心是聚类特征和聚类特征树，特点是在给定内存的情况下，将最小化IO时间作为重要考虑因素，只需对数据扫描一遍
  - 聚类特征
  - 聚类特征树
##### chapter 7数据预测与估算算法    